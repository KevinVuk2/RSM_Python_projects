# -*- coding: utf-8 -*-
"""Assignment_3_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ReJ4NjUAbUsP1a9ythtjzr_OC6iD9Kjo
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import patsy
import statsmodels.api as sm
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score
from sklearn.cluster import KMeans
from scipy.spatial.distance import euclidean, cityblock

import warnings
warnings.filterwarnings("ignore")
pd.options.mode.chained_assignment = None  # default='warn'

#Read CSV file
df = pd.read_csv("insurance.csv")

#Exercise1

#Create "binary_charges"
mu= df['charges'].mean()
df['binary_charges'] = np.where(df['charges'] <= mu, 0, 1)

#Prepare the data for the logistic regression model using patsy to create the design matrix
formula_logit = 'binary_charges ~ age + gender + bmi + children + smoker + region'
y_logit, X_logit = patsy.dmatrices(formula_logit, data=df, return_type='dataframe')

#Build the logistic regression model
logit_model = sm.Logit(y_logit, X_logit).fit()

#Get the summary of the logistic regression model
print(logit_model.summary())
odds_ratio = np.exp(logit_model.params)
print(odds_ratio)

#Predict probabilities
y_probs = logit_model.predict(X_logit)

#Classify using thresholds 0.2 and 0.8
y_pred_02 = (y_probs >= 0.2).astype(int)
y_pred_08 = (y_probs >= 0.8).astype(int)

#a)

#Confusion matrix for threshold 0.2
conf_matrix_02 = confusion_matrix(y_logit, y_pred_02)
print("Confusion Matrix for threshold 0.2:")
print(conf_matrix_02)

#b)
#Confusion matrix for threshold 0.8
conf_matrix_08 = confusion_matrix(y_logit, y_pred_08)
print("Confusion Matrix for threshold 0.8:")
print(conf_matrix_08)

#c)

#Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_logit, y_probs)
roc_auc = auc(fpr, tpr)
print(f"AUC: {roc_auc}")

#Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Logistic Regression')
plt.legend(loc='lower right')
plt.show()

#Exercise2

# Classify using threshold 0.5
y_pred_05 = (y_probs >= 0.5).astype(int)

#a)

# Confusion Matrix for threshold 0.5
conf_matrix_05 = confusion_matrix(y_logit, y_pred_05)
print("Confusion Matrix for threshold 0.5:")
print(conf_matrix_05)

#b)

# Calculate accuracy
accuracy = accuracy_score(y_logit, y_pred_05)
print(f"Accuracy: {accuracy:.4f}")

#c)

# Calculate precision
precision = precision_score(y_logit, y_pred_05)
print(f"Precision: {precision:.4f}")

#d)

# Calculate sensitivity (recall)
sensitivity = recall_score(y_logit, y_pred_05)
print(f"Sensitivity: {sensitivity:.4f}")

#e)

# Calculate specificity
TN, FP, FN, TP = conf_matrix_05.ravel()
specificity = TN / (TN + FP)
print(f"Specificity: {specificity:.4f}")

#f)

# True Positive Rate (TPR) is the same as Sensitivity
TPR = sensitivity
print(f"True Positive Rate: {TPR:.4f}")

#g)

# False Positive Rate (FPR)
FPR = FP / (FP + TN)
print(f"False Positive Rate: {FPR:.4f}")

#Read CSV file
book_ds = pd.read_csv("books.csv")

#Exercise3

#a)

#Euclidean distance between customers 245 and 431
customer_245 = book_ds.iloc[244, 1:]  # Exclude customer_id
customer_431 = book_ds.iloc[430, 1:]
euclidean_distance = np.linalg.norm(customer_245 - customer_431)
print(f"Euclidean Distance: {euclidean_distance:.4f}")
euclidean_distance = np.linalg.norm(customer_245 - customer_431)
print(f"Euclidean Distance between customers 245 and 431: {euclidean_distance:.4f}")

# b)

#Manhattan distance between customers 82 and 197
customer_82 = book_ds.iloc[81, 1:]
customer_197 = book_ds.iloc[196, 1:]
manhattan_distance = np.sum(np.abs(customer_82 - customer_197))
print(f"Manhattan Distance: {manhattan_distance:.4f}")
print(f"Manhattan Distance between customers 82 and 197: {manhattan_distance:.4f}")

#c)

#Centroid of the first 50 customers
first_50_customers = book_ds.drop(columns=['customer_id', 'total_books_purchased'], errors='ignore').iloc[:50]
centroid = first_50_customers.mean(axis=0)
print("Centroid of the first 50 customers:")
centroid

#Exercise4

#Exclude non-genre columns (including 'total_books_purchased') before creating the co-occurrence matrix
book_ds_dropped = book_ds.drop(columns=['customer_id', 'total_books_purchased'], errors='ignore')

#Compute the co-occurrence matrix by multiplying the transaction data matrix with its transpose
co_occurrence_matrix_high = np.dot(book_ds_dropped.T, book_ds_dropped)
co_occurrence_matrix_low = np.dot(book_ds_dropped.T, book_ds_dropped)

#Set the diagonal to 0 to exclude self-co-occurrence (a genre with itself)
np.fill_diagonal(co_occurrence_matrix_high, 0)

#Convert the matrix to a pandas DataFrame for easier readability
co_occurrence_df_high = pd.DataFrame(co_occurrence_matrix_high, columns=book_ds_dropped.columns, index=book_ds_dropped.columns)
co_occurrence_df_low = pd.DataFrame(co_occurrence_matrix_low, columns=book_ds_dropped.columns, index=book_ds_dropped.columns)

#a)

#Find the pair of genres with the highest co-occurrence
max_co_occurrence = co_occurrence_df_high.stack().idxmax()
max_co_occurrence_value = co_occurrence_df_high.stack().max()
print(f"The pair of genres with the highest co-occurrence: {max_co_occurrence} with a value of {max_co_occurrence_value}")

#b)

#Find the pair of genres with the lowest co-occurrence
min_co_occurrence = co_occurrence_df_low.stack().idxmin()
min_co_occurrence_value = co_occurrence_df_low.stack().min()
print(f"The pair of genres with the lowest co-occurrence: {min_co_occurrence} with a value of {min_co_occurrence_value}")

#Exercise5

#Calculate the total number of books purchased by each customer
book_ds['total_books_purchased'] = book_ds.sum(axis=1)

#Apply K-means clustering on the total books purchased
#Reshape the data to be a 2D array
kmeans = KMeans(n_clusters=5)
book_ds['cluster'] = kmeans.fit_predict(book_ds[['total_books_purchased']])

#Calculate the size of each cluster
cluster_sizes = book_ds['cluster'].value_counts()

#Print the size of each cluster
print("Cluster Sizes:")
print(cluster_sizes)

#Exercise6

#Calculate Total number of transactions (customers)
total_transactions = len(book_ds)

#a)

#Support of {fiction}
support_fiction = book_ds['fiction'].sum() / total_transactions
print(f"Support of {{fiction}}: {support_fiction:.4f}")

#b)

#Support of {non_fiction}
support_non_fiction = book_ds['non_fiction'].sum() / total_transactions
print(f"Support of {{non_fiction}}: {support_non_fiction:.4f}")

#c)

#Support of {fiction, self_help}
support_fiction_self_help = ((book_ds['fiction'] == 1) & (book_ds['self_help'] == 1)).sum() / total_transactions
print(f"Support of {{fiction, self_help}}: {support_fiction_self_help:.4f}")

#Exercise7

#a)

#Support of {mystery}
support_mystery = book_ds['mystery'].sum() / total_transactions

#Support of {fiction, mystery}
support_fiction_mystery = ((book_ds['fiction'] == 1) & (book_ds['mystery'] == 1)).sum() / total_transactions

#Calculate the confidence
confidence_fiction_to_mystery = support_fiction_mystery / support_fiction if support_fiction > 0 else 0
print(f"Confidence({{fiction}} → {{mystery}}): {confidence_fiction_to_mystery:.4f}")

#b)

# Support of {non_fiction, self_help}
support_non_fiction_self_help = ((book_ds['non_fiction'] == 1) & (book_ds['self_help'] == 1)).sum() / total_transactions

#Calculate the confidence
confidence_non_fiction_to_self_help = support_non_fiction_self_help / support_non_fiction if support_non_fiction > 0 else 0
print(f"Confidence({{non_fiction}} → {{self_help}}): {confidence_non_fiction_to_self_help:.4f}")

#c)

#Support of {fiction, self_help, childrens_book}
support_fiction_self_help_childrens_books = ((book_ds['fiction'] == 1) &
                                             (book_ds['self_help'] == 1) &
                                             (book_ds['childrens_book'] == 1)).sum() / total_transactions

#Calculate the confidence
confidence_fiction_self_help_to_childrens_books = support_fiction_self_help_childrens_books / support_fiction_self_help if support_fiction_self_help > 0 else 0
print(f"Confidence({{fiction, self_help}} → {{childrens_books}}): {confidence_fiction_self_help_to_childrens_books:.4f}")

#Exercise8

#a)

#Calculate the support
support_childrens_books = book_ds['childrens_book'].sum() / total_transactions

#Calculate the confidence
confidence_fiction_self_help_to_childrens_books = support_fiction_self_help_childrens_books / support_fiction_self_help if support_fiction_self_help > 0 else 0

#Calculate the lift
lift_fiction_self_help_to_childrens_books = confidence_fiction_self_help_to_childrens_books / support_childrens_books if support_childrens_books > 0 else 0
print(f"Lift({{fiction, self_help}} → {{childrens_books}}): {lift_fiction_self_help_to_childrens_books:.4f}")

#b)

#Calculate the support for fiction
support_fiction = book_ds['fiction'].sum() / total_transactions

#Calculate the support for non_fiction
support_non_fiction = book_ds['non_fiction'].sum() / total_transactions

#Calculate the support for {fiction, non_fiction}
support_fiction_non_fiction = ((book_ds['fiction'] == 1) & (book_ds['non_fiction'] == 1)).sum() / total_transactions

#Calculate the confidence for fiction → non_fiction
confidence_fiction_to_non_fiction = support_fiction_non_fiction / support_fiction if support_fiction > 0 else 0

#Calculate the lift
lift_fiction_to_non_fiction = confidence_fiction_to_non_fiction / support_non_fiction if support_non_fiction > 0 else 0
print(f"Lift({{fiction}} → {{non_fiction}}): {lift_fiction_to_non_fiction:.4f}")

#c)

#Calculate the support
support_self_help = book_ds['self_help'].sum() / total_transactions

#Support of {non_fiction, self_help}
support_non_fiction_self_help = ((book_ds['non_fiction'] == 1) & (book_ds['self_help'] == 1)).sum() / total_transactions

#Calculate the confidence
confidence_non_fiction_to_self_help = support_non_fiction_self_help / support_non_fiction if support_non_fiction > 0 else 0

#Calculate the lift
lift_non_fiction_to_self_help = confidence_non_fiction_to_self_help / support_self_help if support_self_help > 0 else 0
print(f"Lift({{non_fiction}} → {{self_help}}): {lift_non_fiction_to_self_help:.4f}")