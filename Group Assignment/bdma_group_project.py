# -*- coding: utf-8 -*-
"""BDMA_group_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u9kHZZ8QsnXGzf68sOhpMvqXJw6KvIgU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, roc_curve, auc

#Read csv file 1
df_1 = pd.read_csv('ebi_base_customers.csv',sep=',')
df_1

#C)

#Transform categorials columns to binary
label_enc = LabelEncoder()
df_1['country'] = label_enc.fit_transform(df_1['country'])
df_1['gender'] = label_enc.fit_transform(df_1['gender'])

#Removing 'customer_id' since it is not relevant for analysis
df_1 = df_1.drop('customer_id', axis=1)

#Handle missing values
df_1.fillna(df_1.mean(), inplace=True)

#Identify and handle outliers using the IQR method
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        #filtering out outliers
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

# Apply IQR method to numeric columns
numeric_cols = df_1.select_dtypes(include=[np.number]).columns.drop('churn')

df_1_filtered = remove_outliers_iqr(df_1, numeric_cols)

df_1_filtered

#D)

#Calculate overall churn rate
churn_rate_og = df_1['churn'].mean()*100
print(f'churn_rate_og: {churn_rate_og:.2f}%')
churn_rate_filtered = df_1_filtered['churn'].mean()*100
print(f'churn_rate_filtered: {churn_rate_filtered:.2f}%')

#Churn rate by demographic variables (age, gender, country,tenure)
churn_by_age = df_1_filtered.groupby('age')['churn'].mean() * 100
churn_by_gender = df_1_filtered.groupby('gender')['churn'].mean() * 100
churn_by_country = df_1_filtered.groupby('country')['churn'].mean() * 100
churn_by_tenure = df_1_filtered.groupby('tenure')['churn'].mean() * 100

print("\nChurn Rate by Age:")
print(churn_by_age)
print("\nChurn Rate by Gender:")
print(churn_by_gender)
print("\nChurn Rate by Country:")
print(churn_by_country)

#Plotting churn rate by age
grid = sns.histplot(data=df_1_filtered, x='age', hue='churn', multiple='stack', kde=False)
grid.set_title('Churn Rate by Age')
plt.show()

#Plotting churn rate by country
grid = sns.barplot(x=churn_by_country.index, y=churn_by_country.values)
grid.set_title('Churn Rate by Country')
plt.show()

#Plotting churn rate by tenure to show relationship
grid = sns.lineplot(x=churn_by_tenure.index, y=churn_by_tenure.values)
grid.set_title('Churn Rate by Tenure')
plt.show()

#Identify patterns
corr_matrix = df_1_filtered.corr()
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

#E)

# Splitting data into features and target
X = df_1_filtered.drop(columns=['churn'])
y = df_1_filtered['churn']
print(f"Features shape: {X.shape}, Target shape: {y.shape}")

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#Model building
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}

#Train and evaluate each model
results = {}
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test)[:, 1]  #Get probabilities for ROC curve

    #Calculate performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    #Calculate ROC curve and AUC
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
    auc_score = auc(fpr, tpr)
    # Store the results in the dictionary
    results[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'AUC': auc_score
    }
    print(f"\n{model_name} Performance:")
    print(classification_report(y_test, y_pred))

 #Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  #Diagonal line (no discrimination)
plt.title(f"ROC Curve - {model_name}")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

#Compare models
results_df = pd.DataFrame(results)
print(results_df)

#Extract feature importances from the Random Forest model
importances = models['Random Forest'].feature_importances_
feature_names = X.columns

#Create a DataFrame for feature importance
print("Number of features:", len(feature_names))
print("Number of importances:", len(importances))
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print(feature_importance_df)

#Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance (Random Forest)')
plt.show()

#F)

#Read csv file 2
df_2 = pd.read_csv('ebi_exp_customers.csv',delimiter=';')

#Preprocess the new data (align with the training data preprocessing)
df_2 = df_2.drop('customer_id', axis=1)

#Transform categorials columns to binary
df_2['country'] = label_enc.fit_transform(df_2['country'])
df_2['gender'] = label_enc.fit_transform(df_2['gender'])
df_2.fillna(df_2.mean(), inplace=True)

#Standardize features
X_exp = scaler.transform(df_2)

#Prediction for likelihood of churn for each custome
df_2['churn_probability'] = models['Random Forest'].predict_proba(X_exp)[:, 1]

# Define the profit values for the €5 and €10 cases
cost_to_avoid_churn = 1

# Case 1: Retention value €5
value_retained_customer_5 = 5
profit_if_retained_5 = value_retained_customer_5 - cost_to_avoid_churn

# Case 2: Retention value €10
value_retained_customer_10 = 10
profit_if_retained_10 = value_retained_customer_10 - cost_to_avoid_churn

# Define thresholds for churn prediction (e.g., 0.6, 0.7, 0.8)
thresholds = [0.6, 0.7, 0.8]

# Dictionary to store results based on thresholds
threshold_results_5 = {}
threshold_results_10 = {}

# Calculate the expected value (EV) and total expected profit for each threshold
for threshold in thresholds:
    # Filter customers with high likelihood of churn (those predicted with churn_probability > threshold)
    df_2['target_customer'] = np.where(df_2['churn_probability'] > threshold, 1, 0)

    # Calculate the expected value (EV) for each customer for the €5 retention case
    df_2['expected_value_5'] = df_2['target_customer'] * profit_if_retained_5

    # Filter customers to target (those with EV > 0)
    high_risk_customers_5 = df_2[df_2['target_customer'] == 1]

    # Calculate total expected profit for customers targeted at this threshold (for €5 retention)
    total_expected_profit_5 = len(high_risk_customers_5) * profit_if_retained_5
    threshold_results_5[threshold] = {
        'Number of Customers to Target': len(high_risk_customers_5),
        'Total Expected Profit': total_expected_profit_5,
        'High Risk Customers': high_risk_customers_5[['churn_probability', 'expected_value_5']]
    }

    # Calculate the expected value (EV) for each customer for the €10 retention case
    df_2['expected_value_10'] = df_2['target_customer'] * profit_if_retained_10

    # Filter customers to target (those with EV > 0)
    high_risk_customers_10 = df_2[df_2['target_customer'] == 1]

    # Calculate total expected profit for customers targeted at this threshold (for €10 retention)
    total_expected_profit_10 = len(high_risk_customers_10) * profit_if_retained_10
    threshold_results_10[threshold] = {
        'Number of Customers to Target': len(high_risk_customers_10),
        'Total Expected Profit': total_expected_profit_10,
        'High Risk Customers': high_risk_customers_10[['churn_probability', 'expected_value_10']]
    }

# Print the results for each threshold and both retention values (€5 and €10)
for threshold, result in threshold_results_5.items():
    print(f"\nThreshold: {threshold} (Retention Value = €5)")
    print(f"Number of Customers to Target: {result['Number of Customers to Target']}")
    print(f"Total Expected Profit: €{result['Total Expected Profit']}")
    print(f"Customers to Target:")
    print(result['High Risk Customers'])

for threshold, result in threshold_results_10.items():
    print(f"\nThreshold: {threshold} (Retention Value = €10)")
    print(f"Number of Customers to Target: {result['Number of Customers to Target']}")
    print(f"Total Expected Profit: €{result['Total Expected Profit']}")
    print(f"Customers to Target:")
    print(result['High Risk Customers'])